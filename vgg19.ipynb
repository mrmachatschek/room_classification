{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "                                                    Path      File_Name  \\\n5606   C:\\Users\\Michael\\Feels Like Home\\Francisco Cru...   601_12_3.jpg   \n6309   C:\\Users\\Michael\\Feels Like Home\\Francisco Cru...   640_11_1.jpg   \n11520  C:\\Users\\Michael\\Feels Like Home\\Francisco Cru...  912_005_1.jpg   \n9260   C:\\Users\\Michael\\Feels Like Home\\Francisco Cru...    796_3_3.jpg   \n16     C:\\Users\\Michael\\Feels Like Home\\Francisco Cru...      1_3_3.jpg   \n...                                                  ...            ...   \n10690  C:\\Users\\Michael\\Feels Like Home\\Francisco Cru...    860_4_3.jpg   \n2524   C:\\Users\\Michael\\Feels Like Home\\Francisco Cru...   391_13_3.jpg   \n2179   C:\\Users\\Michael\\Feels Like Home\\Francisco Cru...    361_5_1.jpg   \n2917   C:\\Users\\Michael\\Feels Like Home\\Francisco Cru...    429_3_1.jpg   \n8536   C:\\Users\\Michael\\Feels Like Home\\Francisco Cru...   766_16_3.jpg   \n\n       ClientId                                          Full_Path Cat  \n5606        601  C:\\Users\\Michael\\Feels Like Home\\Francisco Cru...   3  \n6309        640  C:\\Users\\Michael\\Feels Like Home\\Francisco Cru...   1  \n11520       912  C:\\Users\\Michael\\Feels Like Home\\Francisco Cru...   1  \n9260        796  C:\\Users\\Michael\\Feels Like Home\\Francisco Cru...   3  \n16            1  C:\\Users\\Michael\\Feels Like Home\\Francisco Cru...   3  \n...         ...                                                ...  ..  \n10690       860  C:\\Users\\Michael\\Feels Like Home\\Francisco Cru...   3  \n2524        391  C:\\Users\\Michael\\Feels Like Home\\Francisco Cru...   3  \n2179        361  C:\\Users\\Michael\\Feels Like Home\\Francisco Cru...   1  \n2917        429  C:\\Users\\Michael\\Feels Like Home\\Francisco Cru...   1  \n8536        766  C:\\Users\\Michael\\Feels Like Home\\Francisco Cru...   3  \n\n[4374 rows x 5 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Path</th>\n      <th>File_Name</th>\n      <th>ClientId</th>\n      <th>Full_Path</th>\n      <th>Cat</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>5606</td>\n      <td>C:\\Users\\Michael\\Feels Like Home\\Francisco Cru...</td>\n      <td>601_12_3.jpg</td>\n      <td>601</td>\n      <td>C:\\Users\\Michael\\Feels Like Home\\Francisco Cru...</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <td>6309</td>\n      <td>C:\\Users\\Michael\\Feels Like Home\\Francisco Cru...</td>\n      <td>640_11_1.jpg</td>\n      <td>640</td>\n      <td>C:\\Users\\Michael\\Feels Like Home\\Francisco Cru...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <td>11520</td>\n      <td>C:\\Users\\Michael\\Feels Like Home\\Francisco Cru...</td>\n      <td>912_005_1.jpg</td>\n      <td>912</td>\n      <td>C:\\Users\\Michael\\Feels Like Home\\Francisco Cru...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <td>9260</td>\n      <td>C:\\Users\\Michael\\Feels Like Home\\Francisco Cru...</td>\n      <td>796_3_3.jpg</td>\n      <td>796</td>\n      <td>C:\\Users\\Michael\\Feels Like Home\\Francisco Cru...</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <td>16</td>\n      <td>C:\\Users\\Michael\\Feels Like Home\\Francisco Cru...</td>\n      <td>1_3_3.jpg</td>\n      <td>1</td>\n      <td>C:\\Users\\Michael\\Feels Like Home\\Francisco Cru...</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <td>10690</td>\n      <td>C:\\Users\\Michael\\Feels Like Home\\Francisco Cru...</td>\n      <td>860_4_3.jpg</td>\n      <td>860</td>\n      <td>C:\\Users\\Michael\\Feels Like Home\\Francisco Cru...</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <td>2524</td>\n      <td>C:\\Users\\Michael\\Feels Like Home\\Francisco Cru...</td>\n      <td>391_13_3.jpg</td>\n      <td>391</td>\n      <td>C:\\Users\\Michael\\Feels Like Home\\Francisco Cru...</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <td>2179</td>\n      <td>C:\\Users\\Michael\\Feels Like Home\\Francisco Cru...</td>\n      <td>361_5_1.jpg</td>\n      <td>361</td>\n      <td>C:\\Users\\Michael\\Feels Like Home\\Francisco Cru...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <td>2917</td>\n      <td>C:\\Users\\Michael\\Feels Like Home\\Francisco Cru...</td>\n      <td>429_3_1.jpg</td>\n      <td>429</td>\n      <td>C:\\Users\\Michael\\Feels Like Home\\Francisco Cru...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <td>8536</td>\n      <td>C:\\Users\\Michael\\Feels Like Home\\Francisco Cru...</td>\n      <td>766_16_3.jpg</td>\n      <td>766</td>\n      <td>C:\\Users\\Michael\\Feels Like Home\\Francisco Cru...</td>\n      <td>3</td>\n    </tr>\n  </tbody>\n</table>\n<p>4374 rows Ã— 5 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "#https://stackoverflow.com/questions/42654961/creating-pandas-dataframe-from-os\n",
    "res = []\n",
    "folder_dir = 'C:\\\\Users\\\\Michael\\\\Feels Like Home\\\\Francisco Cruz - Fotos_DL\\\\011_Fotos\\\\'\n",
    "for root, dirs, files in os.walk(folder_dir, topdown=True):\n",
    "    if len(files) > 0:\n",
    "        res.extend(list(zip([root]*len(files), files)))\n",
    "\n",
    "df = pd.DataFrame(res, columns=['Path', 'File_Name'])\n",
    "\n",
    "\n",
    "df = df[df['File_Name'] != 'Thumbs.db']\n",
    "df['ClientId'] = df.Path.apply(lambda x: int(x.split(\"\\\\\")[-1]))\n",
    "df = df[df['ClientId'] < 10000]\n",
    "\n",
    "df['Full_Path'] = df[\"Path\"] + '\\\\' + df[\"File_Name\"]\n",
    "df['Cat'] = df.File_Name.apply(lambda x: x.split(\".\")[0].split(\"_\")[-1])\n",
    "\n",
    "df = df[df.Cat.isin(['1','3','4'])]\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "df_train, df_test = train_test_split(df, test_size=0.2)\n",
    "df_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using VGG19 for feature extraction\n",
    "Freeze the convolutional base and only train the densely connected network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Model: \"vgg19\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\ninput_2 (InputLayer)         (None, 150, 150, 3)       0         \n_________________________________________________________________\nblock1_conv1 (Conv2D)        (None, 150, 150, 64)      1792      \n_________________________________________________________________\nblock1_conv2 (Conv2D)        (None, 150, 150, 64)      36928     \n_________________________________________________________________\nblock1_pool (MaxPooling2D)   (None, 75, 75, 64)        0         \n_________________________________________________________________\nblock2_conv1 (Conv2D)        (None, 75, 75, 128)       73856     \n_________________________________________________________________\nblock2_conv2 (Conv2D)        (None, 75, 75, 128)       147584    \n_________________________________________________________________\nblock2_pool (MaxPooling2D)   (None, 37, 37, 128)       0         \n_________________________________________________________________\nblock3_conv1 (Conv2D)        (None, 37, 37, 256)       295168    \n_________________________________________________________________\nblock3_conv2 (Conv2D)        (None, 37, 37, 256)       590080    \n_________________________________________________________________\nblock3_conv3 (Conv2D)        (None, 37, 37, 256)       590080    \n_________________________________________________________________\nblock3_conv4 (Conv2D)        (None, 37, 37, 256)       590080    \n_________________________________________________________________\nblock3_pool (MaxPooling2D)   (None, 18, 18, 256)       0         \n_________________________________________________________________\nblock4_conv1 (Conv2D)        (None, 18, 18, 512)       1180160   \n_________________________________________________________________\nblock4_conv2 (Conv2D)        (None, 18, 18, 512)       2359808   \n_________________________________________________________________\nblock4_conv3 (Conv2D)        (None, 18, 18, 512)       2359808   \n_________________________________________________________________\nblock4_conv4 (Conv2D)        (None, 18, 18, 512)       2359808   \n_________________________________________________________________\nblock4_pool (MaxPooling2D)   (None, 9, 9, 512)         0         \n_________________________________________________________________\nblock5_conv1 (Conv2D)        (None, 9, 9, 512)         2359808   \n_________________________________________________________________\nblock5_conv2 (Conv2D)        (None, 9, 9, 512)         2359808   \n_________________________________________________________________\nblock5_conv3 (Conv2D)        (None, 9, 9, 512)         2359808   \n_________________________________________________________________\nblock5_conv4 (Conv2D)        (None, 9, 9, 512)         2359808   \n_________________________________________________________________\nblock5_pool (MaxPooling2D)   (None, 4, 4, 512)         0         \n=================================================================\nTotal params: 20,024,384\nTrainable params: 20,024,384\nNon-trainable params: 0\n_________________________________________________________________\n"
    }
   ],
   "source": [
    "from keras.applications import VGG19\n",
    "\n",
    "conv_base = VGG19(weights=\"imagenet\", include_top=False, input_shape=(150, 150, 3))\n",
    "conv_base.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Option \n",
    "# Run the conv_base once with all the images to obtain the features, save the results as a numpy array and then use it as the input to a standalone, densely conncected classifier\n",
    "# Faster option\n",
    "\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "import numpy as np\n",
    "\n",
    "datagen = ImageDataGenerator(rescale = 1./255)\n",
    "\n",
    "# Function adapted from Deep Learning with Python (Francois Chollet, 2018)\n",
    "def extract_features(df, sample_count, batch_size):\n",
    "    features = np.zeros(shape=(sample_count, 4, 4, 512))\n",
    "    labels = np.zeros(shape=(sample_count,3))\n",
    "    generator = datagen.flow_from_dataframe(dataframe=df.iloc[0:sample_count,:], directory=None, x_col='Full_Path', y_col='Cat',\n",
    "                                            target_size=(150, 150), batch_size=batch_size, class_mode='categorical')\n",
    "    i=0\n",
    "    for inputs_batch, labels_batch in generator: \n",
    "        print(\"Batch:\", i)\n",
    "        features_batch = conv_base.predict(inputs_batch)\n",
    "        features[i * batch_size : (i + 1) * batch_size] = features_batch\n",
    "        labels[i * batch_size : (i + 1) * batch_size] = labels_batch\n",
    "        i += 1\n",
    "        if i * batch_size >= sample_count:\n",
    "            break\n",
    "    return features, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Found 150 validated image filenames belonging to 3 classes.\nBatch: 0\nBatch: 1\nBatch: 2\nBatch: 3\nBatch: 4\nBatch: 5\nBatch: 6\nBatch: 7\nBatch: 8\nBatch: 9\nBatch: 10\nBatch: 11\nBatch: 12\nBatch: 13\nBatch: 14\nFound 20 validated image filenames belonging to 3 classes.\nBatch: 0\nBatch: 1\nBatch: 2\nBatch: 3\nBatch: 4\nBatch: 5\nBatch: 6\nBatch: 7\nBatch: 8\nBatch: 9\n"
    }
   ],
   "source": [
    "# Extract features\n",
    "train_size = 150\n",
    "test_size = 20\n",
    "train_features, train_labels = extract_features(df_train, train_size, 10)\n",
    "test_features, test_labels = extract_features(df_test, test_size, 2)\n",
    "\n",
    "# Flatten\n",
    "train_input = np.reshape(train_features, (train_size, 4 * 4 * 512))\n",
    "test_input = np.reshape(test_features, (test_size, 4 * 4 * 512))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Epoch 1/50\n150/150 [==============================] - 1s 4ms/step - loss: 1.5287 - accuracy: 0.3667\nEpoch 2/50\n150/150 [==============================] - 0s 3ms/step - loss: 1.1573 - accuracy: 0.4733\nEpoch 3/50\n150/150 [==============================] - 0s 3ms/step - loss: 0.9853 - accuracy: 0.5667\nEpoch 4/50\n150/150 [==============================] - 0s 2ms/step - loss: 0.9554 - accuracy: 0.5467\nEpoch 5/50\n150/150 [==============================] - 0s 2ms/step - loss: 0.9690 - accuracy: 0.5600\nEpoch 6/50\n150/150 [==============================] - 0s 2ms/step - loss: 0.8948 - accuracy: 0.5867\nEpoch 7/50\n150/150 [==============================] - 0s 2ms/step - loss: 0.7726 - accuracy: 0.6133\nEpoch 8/50\n150/150 [==============================] - 0s 2ms/step - loss: 0.8239 - accuracy: 0.6667\nEpoch 9/50\n150/150 [==============================] - 0s 2ms/step - loss: 0.8065 - accuracy: 0.6200\nEpoch 10/50\n150/150 [==============================] - 0s 2ms/step - loss: 0.7335 - accuracy: 0.6533\nEpoch 11/50\n150/150 [==============================] - 0s 2ms/step - loss: 0.7125 - accuracy: 0.6867\nEpoch 12/50\n150/150 [==============================] - 0s 2ms/step - loss: 0.6355 - accuracy: 0.7267\nEpoch 13/50\n150/150 [==============================] - 0s 2ms/step - loss: 0.6702 - accuracy: 0.7467\nEpoch 14/50\n150/150 [==============================] - 0s 2ms/step - loss: 0.6163 - accuracy: 0.7333\nEpoch 15/50\n150/150 [==============================] - 0s 2ms/step - loss: 0.5439 - accuracy: 0.7800\nEpoch 16/50\n150/150 [==============================] - 0s 2ms/step - loss: 0.5243 - accuracy: 0.7933\nEpoch 17/50\n150/150 [==============================] - 0s 2ms/step - loss: 0.5111 - accuracy: 0.8267\nEpoch 18/50\n150/150 [==============================] - 0s 2ms/step - loss: 0.5226 - accuracy: 0.8200\nEpoch 19/50\n150/150 [==============================] - 0s 2ms/step - loss: 0.4883 - accuracy: 0.8333\nEpoch 20/50\n150/150 [==============================] - 0s 2ms/step - loss: 0.5187 - accuracy: 0.8133\nEpoch 21/50\n150/150 [==============================] - 0s 2ms/step - loss: 0.4742 - accuracy: 0.8000\nEpoch 22/50\n150/150 [==============================] - 0s 2ms/step - loss: 0.4604 - accuracy: 0.8667\nEpoch 23/50\n150/150 [==============================] - 0s 2ms/step - loss: 0.4022 - accuracy: 0.8667\nEpoch 24/50\n150/150 [==============================] - 0s 2ms/step - loss: 0.3816 - accuracy: 0.8800\nEpoch 25/50\n150/150 [==============================] - 0s 2ms/step - loss: 0.3881 - accuracy: 0.8867\nEpoch 26/50\n150/150 [==============================] - 0s 2ms/step - loss: 0.3643 - accuracy: 0.9333\nEpoch 27/50\n150/150 [==============================] - 0s 2ms/step - loss: 0.3731 - accuracy: 0.8867\nEpoch 28/50\n150/150 [==============================] - 0s 2ms/step - loss: 0.3911 - accuracy: 0.8867\nEpoch 29/50\n150/150 [==============================] - 0s 2ms/step - loss: 0.3290 - accuracy: 0.9133\nEpoch 30/50\n150/150 [==============================] - 0s 2ms/step - loss: 0.3093 - accuracy: 0.9467\nEpoch 31/50\n150/150 [==============================] - 0s 2ms/step - loss: 0.2860 - accuracy: 0.9600\nEpoch 32/50\n150/150 [==============================] - 0s 2ms/step - loss: 0.2814 - accuracy: 0.9600\nEpoch 33/50\n150/150 [==============================] - 0s 2ms/step - loss: 0.2960 - accuracy: 0.9200\nEpoch 34/50\n150/150 [==============================] - 0s 2ms/step - loss: 0.2903 - accuracy: 0.9200\nEpoch 35/50\n150/150 [==============================] - 0s 2ms/step - loss: 0.2992 - accuracy: 0.9267\nEpoch 36/50\n150/150 [==============================] - 0s 2ms/step - loss: 0.2551 - accuracy: 0.9600\nEpoch 37/50\n150/150 [==============================] - 0s 2ms/step - loss: 0.2450 - accuracy: 0.9533\nEpoch 38/50\n150/150 [==============================] - 0s 2ms/step - loss: 0.2468 - accuracy: 0.9600\nEpoch 39/50\n150/150 [==============================] - 0s 2ms/step - loss: 0.2314 - accuracy: 0.9733\nEpoch 40/50\n150/150 [==============================] - 0s 2ms/step - loss: 0.2488 - accuracy: 0.9200\nEpoch 41/50\n150/150 [==============================] - 0s 2ms/step - loss: 0.2092 - accuracy: 0.9667\nEpoch 42/50\n150/150 [==============================] - 0s 2ms/step - loss: 0.2023 - accuracy: 0.9600\nEpoch 43/50\n150/150 [==============================] - 0s 2ms/step - loss: 0.2071 - accuracy: 0.9800\nEpoch 44/50\n150/150 [==============================] - 0s 2ms/step - loss: 0.1950 - accuracy: 0.9733\nEpoch 45/50\n150/150 [==============================] - 0s 2ms/step - loss: 0.1916 - accuracy: 0.9667\nEpoch 46/50\n150/150 [==============================] - 0s 2ms/step - loss: 0.1760 - accuracy: 0.9867\nEpoch 47/50\n150/150 [==============================] - 0s 2ms/step - loss: 0.1719 - accuracy: 0.9800\nEpoch 48/50\n150/150 [==============================] - 0s 2ms/step - loss: 0.1526 - accuracy: 0.9933\nEpoch 49/50\n150/150 [==============================] - 0s 2ms/step - loss: 0.1598 - accuracy: 0.9800\nEpoch 50/50\n150/150 [==============================] - 0s 2ms/step - loss: 0.1440 - accuracy: 0.9933\n"
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras import layers\n",
    "from keras import optimizers\n",
    "\n",
    "model = Sequential()\n",
    "model.add(layers.Dense(256, activation=\"relu\", input_dim=4 * 4 * 512))\n",
    "model.add(layers.Dropout(0.5))\n",
    "model.add(layers.Dense(3, activation=\"softmax\"))\n",
    "\n",
    "model.compile(optimizer=optimizers.RMSprop(lr=2e-5),\n",
    "              loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
    "history = model.fit(train_input, train_labels, epochs=50, batch_size=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "20/20 [==============================] - 0s 3ms/step\naccuracy 89.99999761581421\n"
    }
   ],
   "source": [
    "scores = model.evaluate(test_input, test_labels)\n",
    "print(model.metrics_names[1], scores[1]*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using VGG19 for fine-tuning\n",
    "Unfreeze some of the top layer of the convolutional base and train them with the already trained densely connected network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Model: \"sequential_15\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nvgg19 (Model)                (None, 4, 4, 512)         20024384  \n_________________________________________________________________\nflatten_4 (Flatten)          (None, 8192)              0         \n_________________________________________________________________\ndense_19 (Dense)             (None, 256)               2097408   \n_________________________________________________________________\ndropout_10 (Dropout)         (None, 256)               0         \n_________________________________________________________________\ndense_20 (Dense)             (None, 3)                 771       \n=================================================================\nTotal params: 22,122,563\nTrainable params: 22,122,563\nNon-trainable params: 0\n_________________________________________________________________\n"
    }
   ],
   "source": [
    "# Step 1: Combine convolutional base with densely connected network\n",
    "\n",
    "conv_base = VGG19(weights=\"imagenet\", include_top=False, input_shape=(150, 150, 3))\n",
    "\n",
    "model = Sequential()\n",
    "model.add(conv_base)\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(256, activation=\"relu\"))\n",
    "model.add(layers.Dropout(0.5))\n",
    "model.add(layers.Dense(3, activation=\"softmax\"))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Found 150 validated image filenames belonging to 3 classes.\nFound 20 validated image filenames belonging to 3 classes.\nEpoch 1/5\n15/15 [==============================] - 131s 9s/step - loss: 1.1675 - accuracy: 0.4467\nEpoch 2/5\n15/15 [==============================] - 124s 8s/step - loss: 1.0368 - accuracy: 0.5467\nEpoch 3/5\n15/15 [==============================] - 101s 7s/step - loss: 1.0164 - accuracy: 0.5133\nEpoch 4/5\n15/15 [==============================] - 126s 8s/step - loss: 0.9025 - accuracy: 0.5800\nEpoch 5/5\n15/15 [==============================] - 120s 8s/step - loss: 0.7616 - accuracy: 0.6867\n"
    }
   ],
   "source": [
    "# Step 2: Freeze the convolutional base and train only densely connected network\n",
    "conv_base.trainable = False\n",
    "\n",
    "def get_generator(df, sample_count, batch_size):\n",
    "    return datagen.flow_from_dataframe(dataframe=df.iloc[0:sample_count,:], directory=None, x_col='Full_Path', y_col='Cat',\n",
    "                                       target_size=(150, 150), batch_size=batch_size, class_mode='categorical')\n",
    "\n",
    "train_generator = get_generator(df_train, 150, 10)\n",
    "test_generator = get_generator(df_test, 20, 2)\n",
    "\n",
    "model.compile(optimizer=optimizers.RMSprop(lr=2e-5),\n",
    "              loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
    "\n",
    "history = model.fit_generator(train_generator, steps_per_epoch=15, epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "[0.950821042060852, 0.6818181872367859]"
     },
     "metadata": {},
     "execution_count": 42
    }
   ],
   "source": [
    "# Check accuracy \n",
    "model.evaluate_generator(test_generator, steps=11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Model: \"sequential_15\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nvgg19 (Model)                (None, 4, 4, 512)         20024384  \n_________________________________________________________________\nflatten_4 (Flatten)          (None, 8192)              0         \n_________________________________________________________________\ndense_19 (Dense)             (None, 256)               2097408   \n_________________________________________________________________\ndropout_10 (Dropout)         (None, 256)               0         \n_________________________________________________________________\ndense_20 (Dense)             (None, 3)                 771       \n=================================================================\nTotal params: 22,122,563\nTrainable params: 2,098,179\nNon-trainable params: 20,024,384\n_________________________________________________________________\n"
    }
   ],
   "source": [
    "# Step 3: Set trainable attribute of last n layers of convolutional base to True\n",
    "\n",
    "n = 5\n",
    "conv_base.trainable = False \n",
    "for layer in conv_base.layers[-n:]:\n",
    "    layer.trainable = True\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Found 150 validated image filenames belonging to 3 classes.\nFound 20 validated image filenames belonging to 3 classes.\nEpoch 1/5\n15/15 [==============================] - 134s 9s/step - loss: 0.7884 - accuracy: 0.6733\nEpoch 2/5\n15/15 [==============================] - 125s 8s/step - loss: 0.7286 - accuracy: 0.6867\nEpoch 3/5\n15/15 [==============================] - 121s 8s/step - loss: 0.6631 - accuracy: 0.6933\nEpoch 4/5\n15/15 [==============================] - 111s 7s/step - loss: 0.6255 - accuracy: 0.7400\nEpoch 5/5\n15/15 [==============================] - 112s 7s/step - loss: 0.5949 - accuracy: 0.7200\n"
    }
   ],
   "source": [
    "# Step 4: Train last layers of convolutional base jointly with densely connected layer \n",
    "\n",
    "def get_generator(df, sample_count, batch_size):\n",
    "    return datagen.flow_from_dataframe(dataframe=df.iloc[0:sample_count,:], directory=None, x_col='Full_Path', y_col='Cat',\n",
    "                                       target_size=(150, 150), batch_size=batch_size, class_mode='categorical')\n",
    "\n",
    "train_generator = get_generator(df_train, 150, 10)\n",
    "test_generator = get_generator(df_test, 20, 2)\n",
    "\n",
    "model.compile(optimizer=optimizers.RMSprop(lr=2e-5),\n",
    "              loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
    "\n",
    "history = model.fit_generator(train_generator, steps_per_epoch=15, epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "[0.571797251701355, 0.8636363744735718]"
     },
     "metadata": {},
     "execution_count": 45
    }
   ],
   "source": [
    "# Check accuracy \n",
    "model.evaluate_generator(test_generator, steps=11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python37464bitmichaelvirtualenv8c63a04aa4df46b1aed515199121a56c",
   "display_name": "Python 3.7.4 64-bit ('Michael': virtualenv)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}