{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, glob, re\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from pathlib import Path\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from transferlearning import ConvBaseSearch\n",
    "\n",
    "# Glob all jpg-ending files and regex the category out of each\n",
    "p = Path(\"data/*/*\")\n",
    "paths_images = glob.glob(str(p) + \"*jpg\")\n",
    "cat = [re.search(\"\\d+\\.\", i).group().strip(\".\") for i in paths_images]\n",
    "\n",
    "df = pd.DataFrame(list(zip(paths_images,cat)), columns=[\"path\", \"cat\"])\n",
    "df = df[df['cat'].map(df['cat'].value_counts()) > 150]\n",
    "\n",
    "target_col, path_col = \"cat\", \"path\"\n",
    "\n",
    "pretrained_models = [\"vgg16\", \"vgg19\"] #Models that should be tested"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize ConvBaseSearch and pass the model names you want to test\n",
    "cbs = ConvBaseSearch(pretrained_models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 30918460.297300883\n",
      "1 26816114.09336842\n",
      "2 26627740.458248965\n",
      "3 25654347.267109454\n",
      "4 21875774.895298693\n",
      "5 15734826.634910446\n",
      "6 9802976.45263368\n",
      "7 5600789.419625288\n",
      "8 3199924.333483309\n",
      "9 1945893.0272589237\n",
      "10 1301375.3442368507\n",
      "11 952060.2376855612\n",
      "12 745278.6015071997\n",
      "13 609747.140243778\n",
      "14 512805.1632633341\n",
      "15 438639.01064992853\n",
      "16 379369.1741590543\n",
      "17 330751.010252889\n",
      "18 290187.5856081849\n",
      "19 255905.86954074336\n",
      "20 226653.00788524354\n",
      "21 201530.52875877568\n",
      "22 179807.63179573725\n",
      "23 160925.2470097556\n",
      "24 144455.93778561143\n",
      "25 130025.55628528679\n",
      "26 117354.22441616254\n",
      "27 106169.89942296047\n",
      "28 96262.75753540755\n",
      "29 87455.70628169496\n",
      "30 79608.69174316981\n",
      "31 72595.03979501214\n",
      "32 66309.07506801258\n",
      "33 60660.7385308112\n",
      "34 55573.46280716207\n",
      "35 50984.081296372155\n",
      "36 46836.66543094514\n",
      "37 43080.78981689315\n",
      "38 39674.433072582324\n",
      "39 36583.654917995416\n",
      "40 33769.9112940329\n",
      "41 31204.70322255723\n",
      "42 28862.226316518376\n",
      "43 26720.892459806935\n",
      "44 24761.116038107426\n",
      "45 22966.80127163222\n",
      "46 21320.605035476754\n",
      "47 19808.35641263392\n",
      "48 18417.322101086\n",
      "49 17138.60135434263\n",
      "50 15960.661181715059\n",
      "51 14874.008737487387\n",
      "52 13870.700794385932\n",
      "53 12943.31879297036\n",
      "54 12085.359076014003\n",
      "55 11291.815531130986\n",
      "56 10556.83901469475\n",
      "57 9875.112202643992\n",
      "58 9242.345627705761\n",
      "59 8654.590302190292\n",
      "60 8108.270440658086\n",
      "61 7600.0240917916635\n",
      "62 7126.968054391907\n",
      "63 6686.219814025795\n",
      "64 6275.504445523597\n",
      "65 5892.812858313084\n",
      "66 5536.084729496474\n",
      "67 5203.061426030834\n",
      "68 4891.863601095793\n",
      "69 4600.991410756865\n",
      "70 4329.070731320146\n",
      "71 4074.5512105022553\n",
      "72 3836.3110923970753\n",
      "73 3613.166648422889\n",
      "74 3404.067886692451\n",
      "75 3208.052192088282\n",
      "76 3024.4034532124197\n",
      "77 2852.4127398327814\n",
      "78 2690.966138223663\n",
      "79 2539.3301253953923\n",
      "80 2396.902256158659\n",
      "81 2263.030562984498\n",
      "82 2137.1863447921896\n",
      "83 2018.8279574319054\n",
      "84 1907.503442008571\n",
      "85 1802.7220696118159\n",
      "86 1704.1062577756686\n",
      "87 1611.2232262428079\n",
      "88 1523.7312954958713\n",
      "89 1441.2832724515324\n",
      "90 1363.5716559578566\n",
      "91 1290.3065812558903\n",
      "92 1221.2153026576457\n",
      "93 1156.1482772249442\n",
      "94 1094.8517477047303\n",
      "95 1036.977212431222\n",
      "96 982.372825722771\n",
      "97 930.7535641292745\n",
      "98 881.9973764238126\n",
      "99 835.9287905980041\n",
      "100 792.3954504818507\n",
      "101 751.2461998448514\n",
      "102 712.3335831603026\n",
      "103 675.4783609646995\n",
      "104 640.6223712554818\n",
      "105 607.6522720398408\n",
      "106 576.4511872141422\n",
      "107 546.9262935004958\n",
      "108 518.98512913029\n",
      "109 492.5346382837992\n",
      "110 467.48287191552737\n",
      "111 443.7543973321889\n",
      "112 421.278367393621\n",
      "113 399.9843217300572\n",
      "114 379.8122113244578\n",
      "115 360.698373055695\n",
      "116 342.5810415865332\n",
      "117 325.4014493249439\n",
      "118 309.11344087307737\n",
      "119 293.67135152118806\n",
      "120 279.0273308147732\n",
      "121 265.13394910149646\n",
      "122 251.9587875775514\n",
      "123 239.4582609230609\n",
      "124 227.59497859758625\n",
      "125 216.33782054155608\n",
      "126 205.65376138054916\n",
      "127 195.51100441246288\n",
      "128 185.88452325418277\n",
      "129 176.746230044453\n",
      "130 168.0704188837221\n",
      "131 159.8292498331371\n",
      "132 152.00224864702415\n",
      "133 144.5689568076833\n",
      "134 137.5089113003687\n",
      "135 130.80420341687176\n",
      "136 124.4321046415429\n",
      "137 118.3782850614242\n",
      "138 112.62502954157384\n",
      "139 107.1578495989085\n",
      "140 101.96377413873066\n",
      "141 97.02704037990313\n",
      "142 92.33366607131619\n",
      "143 87.87175268690456\n",
      "144 83.6299237605321\n",
      "145 79.59680082083628\n",
      "146 75.76333931051154\n",
      "147 72.117480236129\n",
      "148 68.6496691011055\n",
      "149 65.35203013277612\n",
      "150 62.21601172279459\n",
      "151 59.23307452078175\n",
      "152 56.396783949557665\n",
      "153 53.697548495557804\n",
      "154 51.129757811521614\n",
      "155 48.686781767685275\n",
      "156 46.36240599391219\n",
      "157 44.151605485418784\n",
      "158 42.04815807374962\n",
      "159 40.045868223346446\n",
      "160 38.14031190315611\n",
      "161 36.32686208822278\n",
      "162 34.60090648619196\n",
      "163 32.958340824005845\n",
      "164 31.39506255496743\n",
      "165 29.906619142015302\n",
      "166 28.489727822152645\n",
      "167 27.140808686338293\n",
      "168 25.857001394405223\n",
      "169 24.634554093155646\n",
      "170 23.470762403222132\n",
      "171 22.36254125739125\n",
      "172 21.307218414778767\n",
      "173 20.302566890684776\n",
      "174 19.345736884795695\n",
      "175 18.434659159020242\n",
      "176 17.567005416958708\n",
      "177 16.740776125415636\n",
      "178 15.953770095678914\n",
      "179 15.204039228272737\n",
      "180 14.489924732043805\n",
      "181 13.809766607773994\n",
      "182 13.161894049906106\n",
      "183 12.544967438911218\n",
      "184 11.956980622154772\n",
      "185 11.396916449285719\n",
      "186 10.863321984977617\n",
      "187 10.35498848508687\n",
      "188 9.870710586040309\n",
      "189 9.40939902285289\n",
      "190 8.969788354691469\n",
      "191 8.550875059203621\n",
      "192 8.151713506905992\n",
      "193 7.771374323909662\n",
      "194 7.408984729706296\n",
      "195 7.063687230567684\n",
      "196 6.734552493594892\n",
      "197 6.420905893460047\n",
      "198 6.121988209246762\n",
      "199 5.837070090545135\n",
      "200 5.565525808821895\n",
      "201 5.306779554824524\n",
      "202 5.060211082208476\n",
      "203 4.825101588762114\n",
      "204 4.6010089407493355\n",
      "205 4.387437236611918\n",
      "206 4.1838679755313235\n",
      "207 3.9897923369843458\n",
      "208 3.804809040001724\n",
      "209 3.6284506753284647\n",
      "210 3.460335213704454\n",
      "211 3.3000887352866113\n",
      "212 3.1472979848303337\n",
      "213 3.001624057035572\n",
      "214 2.8627505738030967\n",
      "215 2.7303789875726436\n",
      "216 2.604129946234205\n",
      "217 2.4837623855622564\n",
      "218 2.3690124027856103\n",
      "219 2.2595908391079904\n",
      "220 2.155262968963239\n",
      "221 2.055780508951451\n",
      "222 1.9609291926728014\n",
      "223 1.870470618065435\n",
      "224 1.7842187129402158\n",
      "225 1.7019639771010895\n",
      "226 1.6235183039934304\n",
      "227 1.5487223886763724\n",
      "228 1.477409707895213\n",
      "229 1.4093960644083403\n",
      "230 1.3445155798849424\n",
      "231 1.282646798051354\n",
      "232 1.2236418316892346\n",
      "233 1.1673670863386647\n",
      "234 1.1136940345225266\n",
      "235 1.0625101670179706\n",
      "236 1.0136813492231458\n",
      "237 0.9671178728928294\n",
      "238 0.9226959696217947\n",
      "239 0.8803297589900297\n",
      "240 0.8399172713100593\n",
      "241 0.801379107059167\n",
      "242 0.7646170956764868\n",
      "243 0.7295453367550766\n",
      "244 0.6960942713611538\n",
      "245 0.6641903702668166\n",
      "246 0.6337479369597795\n",
      "247 0.6047075990852169\n",
      "248 0.5770080249787579\n",
      "249 0.5505906677651795\n",
      "250 0.5253798906485379\n",
      "251 0.5013280991501164\n",
      "252 0.4783840897913238\n",
      "253 0.4564989500639067\n",
      "254 0.43561859407475445\n",
      "255 0.41569973372359914\n",
      "256 0.39669294575595004\n",
      "257 0.37856208043457407\n",
      "258 0.361261017667013\n",
      "259 0.3447543201777537\n",
      "260 0.3290044201305702\n",
      "261 0.3139816587974743\n",
      "262 0.2996507173882887\n",
      "263 0.28597067399323106\n",
      "264 0.2729184806686118\n",
      "265 0.2604666091097666\n",
      "266 0.24858484966829775\n",
      "267 0.23724663991160044\n",
      "268 0.2264273254266218\n",
      "269 0.21610731448475867\n",
      "270 0.20625703735132045\n",
      "271 0.1968571526907319\n",
      "272 0.18788751346962942\n",
      "273 0.17932914065828023\n",
      "274 0.1711613666202597\n",
      "275 0.16336776767819133\n",
      "276 0.1559325183585189\n",
      "277 0.14883582800943435\n",
      "278 0.14206345563846756\n",
      "279 0.13559928900126536\n",
      "280 0.1294311368809905\n",
      "281 0.12354453431126781\n",
      "282 0.1179265052205554\n",
      "283 0.11256649319912504\n",
      "284 0.10744972296506816\n",
      "285 0.10256670600145659\n",
      "286 0.09790677874261319\n",
      "287 0.09345863652405025\n",
      "288 0.08921380620489552\n",
      "289 0.08516237033440113\n",
      "290 0.08129651770470683\n",
      "291 0.07760601971469164\n",
      "292 0.07408379046866467\n",
      "293 0.07072182929889702\n",
      "294 0.06751369831707142\n",
      "295 0.0644512509944952\n",
      "296 0.06152810887123756\n",
      "297 0.058738652664988684\n",
      "298 0.05607522272580387\n",
      "299 0.05353355097467201\n",
      "300 0.05110721488624132\n",
      "301 0.048790998844031555\n",
      "302 0.0465802608904687\n",
      "303 0.044470422226927396\n",
      "304 0.04245677999716389\n",
      "305 0.04053413591782325\n",
      "306 0.0386987824004874\n",
      "307 0.036946913794603436\n",
      "308 0.03527472128686075\n",
      "309 0.033678361141193824\n",
      "310 0.032154613796608805\n",
      "311 0.030700381749371716\n",
      "312 0.02931171883949165\n",
      "313 0.027986094331153698\n",
      "314 0.026720712096721853\n",
      "315 0.025512545991866642\n",
      "316 0.024359267845147577\n",
      "317 0.02325835612079176\n",
      "318 0.022207599946797902\n",
      "319 0.02120421548069111\n",
      "320 0.020246359459225156\n",
      "321 0.019331917942476375\n",
      "322 0.018458814022716173\n",
      "323 0.01762533849864092\n",
      "324 0.016829592825034125\n",
      "325 0.016070190545020582\n",
      "326 0.015344930077736813\n",
      "327 0.014652412599393434\n",
      "328 0.013991386326742227\n",
      "329 0.013360230810808754\n",
      "330 0.012757544706781806\n",
      "331 0.012182185994796793\n",
      "332 0.011632973873706669\n",
      "333 0.01110853703456638\n",
      "334 0.010607746961313079\n",
      "335 0.010129656698411819\n",
      "336 0.009673125521832674\n",
      "337 0.009237248213770019\n",
      "338 0.008821134138511478\n",
      "339 0.008423802256803139\n",
      "340 0.00804444603555999\n",
      "341 0.007682211493338942\n",
      "342 0.007336326872437973\n",
      "343 0.007006044237321676\n",
      "344 0.0066906627020522046\n",
      "345 0.006389521688377272\n",
      "346 0.006101994672371221\n",
      "347 0.005827509718476503\n",
      "348 0.005565327527588168\n",
      "349 0.00531496856553648\n",
      "350 0.005075935029123833\n",
      "351 0.00484765975569425\n",
      "352 0.004629681991353102\n",
      "353 0.004421544923358718\n",
      "354 0.004222825162470872\n",
      "355 0.0040330352453359156\n",
      "356 0.003851832214701809\n",
      "357 0.0036787424134752376\n",
      "358 0.003513467727864026\n",
      "359 0.0033556434551538328\n",
      "360 0.003204916284906479\n",
      "361 0.0030610019158494\n",
      "362 0.002923569630659058\n",
      "363 0.0027923102608737845\n",
      "364 0.002666950877744976\n",
      "365 0.00254724750739526\n",
      "366 0.002432922716397966\n",
      "367 0.002323735920935502\n",
      "368 0.002219497168595576\n",
      "369 0.0021199474763594984\n",
      "370 0.0020248428077318703\n",
      "371 0.0019340355864384206\n",
      "372 0.0018473081382945548\n",
      "373 0.0017644794893881101\n",
      "374 0.0016853675177686501\n",
      "375 0.0016098160679772235\n",
      "376 0.00153767740195804\n",
      "377 0.0014687603950512358\n",
      "378 0.0014029463255094015\n",
      "379 0.001340087937597134\n",
      "380 0.0012800457604111642\n",
      "381 0.001222707248108751\n",
      "382 0.0011679445326420988\n",
      "383 0.0011156455813967417\n",
      "384 0.0010656968099580799\n",
      "385 0.0010179824581433958\n",
      "386 0.0009724170758518048\n",
      "387 0.0009288893984845966\n",
      "388 0.0008873149184805784\n",
      "389 0.0008476079338135626\n",
      "390 0.0008096837217377824\n",
      "391 0.0007734655800315718\n",
      "392 0.0007388648202880591\n",
      "393 0.0007058153908399813\n",
      "394 0.0006742482013315232\n",
      "395 0.0006440982461968924\n",
      "396 0.0006152979863456527\n",
      "397 0.0005877877922580033\n",
      "398 0.0005615159954085094\n",
      "399 0.0005364227596646482\n",
      "400 0.0005124499447627795\n",
      "401 0.0004895515282858428\n",
      "402 0.0004676792417237852\n",
      "403 0.000446784368497227\n",
      "404 0.0004268245587753082\n",
      "405 0.0004077631309761486\n",
      "406 0.0003895561298500397\n",
      "407 0.00037215889300219285\n",
      "408 0.0003555422268832269\n",
      "409 0.00033966904143979204\n",
      "410 0.0003245056175077781\n",
      "411 0.00031002088063250056\n",
      "412 0.0002961851153913975\n",
      "413 0.00028296976885158824\n",
      "414 0.0002703447879423962\n",
      "415 0.00025828435410138733\n",
      "416 0.00024676353903732996\n",
      "417 0.0002357569101346677\n",
      "418 0.00022524096472540138\n",
      "419 0.0002151965593499718\n",
      "420 0.0002056012689963182\n",
      "421 0.0001964369686620263\n",
      "422 0.0001876796522365179\n",
      "423 0.0001793140357910346\n",
      "424 0.00017132135617718597\n",
      "425 0.00016368587606498033\n",
      "426 0.00015639274045974033\n",
      "427 0.00014942437108400938\n",
      "428 0.00014276785263628712\n",
      "429 0.00013640948752397178\n",
      "430 0.00013033480231101186\n",
      "431 0.0001245299106548462\n",
      "432 0.00011898412984503255\n",
      "433 0.00011368639718682509\n",
      "434 0.00010862502066930235\n",
      "435 0.00010378903503563653\n",
      "436 9.917016167535864e-05\n",
      "437 9.47567606362081e-05\n",
      "438 9.05396882626701e-05\n",
      "439 8.651071461048271e-05\n",
      "440 8.26617461732942e-05\n",
      "441 7.898400108680378e-05\n",
      "442 7.547016279832995e-05\n",
      "443 7.211373740597456e-05\n",
      "444 6.890792699565043e-05\n",
      "445 6.584346193308995e-05\n",
      "446 6.291548023568895e-05\n",
      "447 6.0118177832325853e-05\n",
      "448 5.744574478901082e-05\n",
      "449 5.48920329186218e-05\n",
      "450 5.245212646727391e-05\n",
      "451 5.0121560512169756e-05\n",
      "452 4.789445450959768e-05\n",
      "453 4.576630983143329e-05\n",
      "454 4.373286917800335e-05\n",
      "455 4.179016082432486e-05\n",
      "456 3.993378832473219e-05\n",
      "457 3.816009534460878e-05\n",
      "458 3.6465631023334874e-05\n",
      "459 3.484681915606943e-05\n",
      "460 3.3299502178879204e-05\n",
      "461 3.1821093944382e-05\n",
      "462 3.04086033902879e-05\n",
      "463 2.905870634004837e-05\n",
      "464 2.7768903046474688e-05\n",
      "465 2.653644336432385e-05\n",
      "466 2.535910384056116e-05\n",
      "467 2.4234159535618483e-05\n",
      "468 2.315889173056776e-05\n",
      "469 2.21315371507999e-05\n",
      "470 2.114982930016055e-05\n",
      "471 2.0211701629771552e-05\n",
      "472 1.931551055766023e-05\n",
      "473 1.845898183398752e-05\n",
      "474 1.7640566938954337e-05\n",
      "475 1.6858591410877783e-05\n",
      "476 1.6111160330210028e-05\n",
      "477 1.539705140019321e-05\n",
      "478 1.4714575727460943e-05\n",
      "479 1.4062419618208763e-05\n",
      "480 1.3439231341672254e-05\n",
      "481 1.284373636925796e-05\n",
      "482 1.2274771291259811e-05\n",
      "483 1.1730990679414808e-05\n",
      "484 1.1211387461954143e-05\n",
      "485 1.0714826784923659e-05\n",
      "486 1.0240279266970863e-05\n",
      "487 9.786764940354261e-06\n",
      "488 9.353406277220017e-06\n",
      "489 8.939266831149709e-06\n",
      "490 8.543582874371208e-06\n",
      "491 8.165367656950947e-06\n",
      "492 7.803948084625754e-06\n",
      "493 7.458532953112288e-06\n",
      "494 7.128425396156085e-06\n",
      "495 6.8129475054764944e-06\n",
      "496 6.511508811284314e-06\n",
      "497 6.223410359322073e-06\n",
      "498 5.948155516122294e-06\n",
      "499 5.685040913688878e-06\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import numpy as np\n",
    "\n",
    "# N is batch size; D_in is input dimension;\n",
    "# H is hidden dimension; D_out is output dimension.\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "\n",
    "# Create random input and output data\n",
    "x = np.random.randn(N, D_in)\n",
    "y = np.random.randn(N, D_out)\n",
    "\n",
    "# Randomly initialize weights\n",
    "w1 = np.random.randn(D_in, H)\n",
    "w2 = np.random.randn(H, D_out)\n",
    "\n",
    "learning_rate = 1e-6\n",
    "for t in range(500):\n",
    "    # Forward pass: compute predicted y\n",
    "    h = x.dot(w1)\n",
    "    h_relu = np.maximum(h, 0)\n",
    "    y_pred = h_relu.dot(w2)\n",
    "\n",
    "    # Compute and print loss\n",
    "    loss = np.square(y_pred - y).sum()\n",
    "    print(t, loss)\n",
    "\n",
    "    # Backprop to compute gradients of w1 and w2 with respect to loss\n",
    "    grad_y_pred = 2.0 * (y_pred - y)\n",
    "    grad_w2 = h_relu.T.dot(grad_y_pred)\n",
    "    grad_h_relu = grad_y_pred.dot(w2.T)\n",
    "    grad_h = grad_h_relu.copy()\n",
    "    grad_h[h < 0] = 0\n",
    "    grad_w1 = x.T.dot(grad_h)\n",
    "\n",
    "    # Update weights\n",
    "    w1 -= learning_rate * grad_w1\n",
    "    w2 -= learning_rate * grad_w2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99 749.316650390625\n",
      "199 5.224360942840576\n",
      "299 0.0619528591632843\n",
      "399 0.0012128647649660707\n",
      "499 0.00011697161971824244\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "dtype = torch.float\n",
    "device = torch.device(\"cpu\")\n",
    "# device = torch.device(\"cuda:0\") # Uncomment this to run on GPU\n",
    "\n",
    "# N is batch size; D_in is input dimension;\n",
    "# H is hidden dimension; D_out is output dimension.\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "\n",
    "# Create random input and output data\n",
    "x = torch.randn(N, D_in, device=device, dtype=dtype)\n",
    "y = torch.randn(N, D_out, device=device, dtype=dtype)\n",
    "\n",
    "# Randomly initialize weights\n",
    "w1 = torch.randn(D_in, H, device=device, dtype=dtype)\n",
    "w2 = torch.randn(H, D_out, device=device, dtype=dtype)\n",
    "\n",
    "learning_rate = 1e-6\n",
    "for t in range(500):\n",
    "    # Forward pass: compute predicted y\n",
    "    h = x.mm(w1)\n",
    "    h_relu = h.clamp(min=0)\n",
    "    y_pred = h_relu.mm(w2)\n",
    "\n",
    "    # Compute and print loss\n",
    "    loss = (y_pred - y).pow(2).sum().item()\n",
    "    if t % 100 == 99:\n",
    "        print(t, loss)\n",
    "\n",
    "    # Backprop to compute gradients of w1 and w2 with respect to loss\n",
    "    grad_y_pred = 2.0 * (y_pred - y)\n",
    "    grad_w2 = h_relu.t().mm(grad_y_pred)\n",
    "    grad_h_relu = grad_y_pred.mm(w2.t())\n",
    "    grad_h = grad_h_relu.clone()\n",
    "    grad_h[h < 0] = 0\n",
    "    grad_w1 = x.t().mm(grad_h)\n",
    "\n",
    "    # Update weights using gradient descent\n",
    "    w1 -= learning_rate * grad_w1\n",
    "    w2 -= learning_rate * grad_w2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 163 validated image filenames belonging to 12 classes.\n",
      "Found 41 validated image filenames belonging to 12 classes.\n",
      "Fitting  vgg16\n",
      "Epoch 1/1\n"
     ]
    },
    {
     "ename": "UnknownError",
     "evalue": " Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above.\n\t [[node vgg16/block1_conv1/convolution (defined at /home/d4ve/.local/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3009) ]] [Op:__inference_keras_scratch_graph_1617]\n\nFunction call stack:\nkeras_scratch_graph\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnknownError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-f4544a625b60>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Fit the data to the pretrained models. Only use sample_size proportion of whole dataset.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mcbs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath_col\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_col\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.02\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_augmentation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/nova/room_classification/transferlearning.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, df, path_col, target_col, sample_size, data_augmentation)\u001b[0m\n\u001b[1;32m    137\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Fitting \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconv_base\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m                 self.fit_with_augmentation(\n\u001b[0;32m--> 139\u001b[0;31m                     \u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconv_base\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_col\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_generator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_generator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    140\u001b[0m                 )\n\u001b[1;32m    141\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Score on test set: \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconv_base\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/nova/room_classification/transferlearning.py\u001b[0m in \u001b[0;36mfit_with_augmentation\u001b[0;34m(self, df, conv_base, target_col, train_generator, test_generator)\u001b[0m\n\u001b[1;32m    196\u001b[0m         )\n\u001b[1;32m    197\u001b[0m         model.fit_generator(\n\u001b[0;32m--> 198\u001b[0;31m             \u001b[0mtrain_generator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mround\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m0.8\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m0.01\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m//\u001b[0m \u001b[0;36m50\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    199\u001b[0m         )\n\u001b[1;32m    200\u001b[0m         \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate_generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_generator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name + '` call to the ' +\n\u001b[1;32m     90\u001b[0m                               'Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   1730\u001b[0m             \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1731\u001b[0m             \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1732\u001b[0;31m             initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1733\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/keras/engine/training_generator.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(model, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m    218\u001b[0m                                             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m                                             \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 220\u001b[0;31m                                             reset_metrics=False)\n\u001b[0m\u001b[1;32m    221\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    222\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight, reset_metrics)\u001b[0m\n\u001b[1;32m   1512\u001b[0m             \u001b[0mins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1513\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1514\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1515\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1516\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mreset_metrics\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/keras/backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   3790\u001b[0m         \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmath_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3791\u001b[0m       \u001b[0mconverted_inputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3792\u001b[0;31m     \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_graph_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mconverted_inputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3793\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3794\u001b[0m     \u001b[0;31m# EagerTensor.numpy() will often make a copy to ensure memory safety.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1603\u001b[0m       \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mFor\u001b[0m \u001b[0minvalid\u001b[0m \u001b[0mpositional\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mkeyword\u001b[0m \u001b[0margument\u001b[0m \u001b[0mcombinations\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1604\u001b[0m     \"\"\"\n\u001b[0;32m-> 1605\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1606\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1607\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, args, kwargs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1643\u001b[0m       raise TypeError(\"Keyword arguments {} unknown. Expected {}.\".format(\n\u001b[1;32m   1644\u001b[0m           list(kwargs.keys()), list(self._arg_keywords)))\n\u001b[0;32m-> 1645\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1646\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1647\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1744\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1745\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1746\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1747\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1748\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    596\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    597\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 598\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    599\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    600\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mUnknownError\u001b[0m:  Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above.\n\t [[node vgg16/block1_conv1/convolution (defined at /home/d4ve/.local/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3009) ]] [Op:__inference_keras_scratch_graph_1617]\n\nFunction call stack:\nkeras_scratch_graph\n"
     ]
    }
   ],
   "source": [
    "# Fit the data to the pretrained models. Only use sample_size proportion of whole dataset.\n",
    "cbs.fit(df, path_col, target_col, sample_size=0.02, data_augmentation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2222222238779068"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the best score \n",
    "cbs.best_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: vgg16\n",
      "Keras model: <keras.engine.training.Model object at 0x000002C074E10F88>\n"
     ]
    }
   ],
   "source": [
    "# Get the best model\n",
    "best_base = cbs.best_base\n",
    "print(\"Name:\", best_base.name)\n",
    "print(\"Keras model:\", best_base.model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
